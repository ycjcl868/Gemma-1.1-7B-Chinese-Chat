# Gemma-1.1-7B-Chinese-Chat
Chinese chat model specifically fine-tuned for Chinese through SFT based on the gemma-1.1-7b-it model.

# Updates
- [May 23, 2024] ğŸ”¥ support function calling


# Model Summary

[Gemma-1.1-7B-Chinese-Chat](https://huggingface.co/ycjcl868/Gemma-1.1-7B-Chinese-Chat) is an instruction-tuned language model for Chinese & English users built upon the gemma-1.1-7b-it model.([Github](https://github.com/ycjcl868/Gemma-1.1-7B-Chinese-Chat/tree/main))

Developed by: [ycjcl868](https://github.com/ycjcl868)

- License: [Gemma License](https://www.kaggle.com/models/google/gemma/license/consent)
- Base Model: gemma-1.1-7b-it
- Model Size: 8.54B
- Context length: 8K

# Introduction

This is the first model specifically fine-tuned for Chinese & English user through SFT based on the [gemma-1.1-7b-it model](https://huggingface.co/google/gemma-1.1-7b-it).

**Compared to the original [gemma-1.1-7b-it model](https://huggingface.co/google/gemma-1.1-7b-it), our Gemma-1.1-7B-Chinese-Chat model significantly reduces the issues of "Chinese questions with English answers" and the mixing of Chinese and English in responses.**

Training details:

- epochs: 3
- learning rate: 0.0001
- learning rate scheduler type: cosine
- Warmup ratio: 0.1
- cutoff len (i.e. context length): 8192
- global batch size: 2

<br />

# Usage

## Chat Template

The instruction-tuned models use a chat template that must be adhered to for conversational use.
The easiest way to apply it is using the tokenizer's built-in chat template, as shown in the following snippet.

Let's load the model and apply the chat template to a conversation. In this example, we'll start with a single user interaction:

```py
from transformers import AutoTokenizer, AutoModelForCausalLM
import transformers
import torch

model_id = "ycjcl868/Gemma-1.1-7B-Chinese-Chat"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    torch_dtype="auto",
)

messages = [
    { "role": "user", "content": "å†™ä¸€é¦–è¯—" },
]
input_ids = tokenizer.apply_chat_template(
    messages, add_generation_prompt=True, return_tensors="pt"
).to(model.device)

outputs = model.generate(
    input_ids,
    max_new_tokens=8192,
    do_sample=True,
    temperature=0.6,
    top_p=0.9,
)
response = outputs[0][input_ids.shape[-1]:]
print(tokenizer.decode(response, skip_special_tokens=True))

# æ˜¥é£æ‹‚æŸ³ï¼Œæ˜¥é›¨è½»è½»ï¼Œ
# èŠ±é¦™æ»¡å›­ï¼Œé¸Ÿé¸£æ¸…è„†ã€‚
# ä¸€æœµçº¢æ¢…ï¼Œçº¢å½¤å½¤çš„ï¼Œ
# ç¾ä¸èƒœæ”¶ï¼Œç¾ä¸èƒœæ”¶ã€‚
```

# Examples

```
User: å‚åŠ å››å¹´çº§æ•°å­¦ç«èµ›åŒå­¦ä¸­ï¼Œç”·åŒå­¦è·å¥–äººæ•°æ¯”å¥³åŒå­¦å¤š2äººï¼Œå¥³åŒå­¦è·å¥–äººæ•°æ¯”ç”·åŒå­¦äººæ•°çš„ä¸€åŠå¤š2äººï¼Œç”·å¥³åŒå­¦å„æœ‰å¤šå°‘äººè·å¥–?
Assistant:
è®¾ç”·åŒå­¦çš„è·å¥–äººæ•°
ä¸ºxï¼Œåˆ™å¥³åŒå­¦çš„è·å¥–äººæ•°ä¸ºyï¼Œæ‰€ä»¥æœ‰ä»¥ä¸‹ä¸¤å¼ï¼š
x=y+2
y=2(x+2)
å°†ç¬¬ä¸€å¼ä»£å…¥ç¬¬äºŒå¼ä¸­ï¼Œå¾—ï¼š
y=2(y+2)+2
y=2y+4+2
y=2y+6
2y=y+6
y=6
å› æ­¤ï¼Œå¥³åŒå­¦è·å¥–äººæ•°ä¸º6äººï¼Œç”·åŒå­¦è·å¥–äººæ•°ä¸º8äººã€‚
```

```
User: æˆ‘çš„è“ç‰™è€³æœºåäº†ï¼Œæˆ‘åº”è¯¥å»çœ‹è€³é¼»å–‰ç§‘è¿˜æ˜¯ç‰™ç§‘ï¼Ÿ
Assistant: è“ç‰™è€³æœºæ˜¯å±äºç”µå­äº§å“çš„ï¼Œè€Œä¸æ˜¯è€³é¼»å–‰ç§‘æˆ–ç‰™ç§‘çš„èŒƒç•´ã€‚å¦‚æœä½ æ€€ç–‘è€³æœºæœ‰æ•…éšœï¼Œå»ºè®®å»ç”µå­äº§å“ç»´ä¿®åº—æˆ–è€…åŸå‚ç»´ä¿®ä¸­å¿ƒè¿›è¡Œæ£€æŸ¥ã€‚å¦‚æœéœ€è¦æ–°çš„è€³æœºï¼Œä½ ä¹Ÿå¯ä»¥å»ç”µå™¨å•†åº—æˆ–ç”µå­äº§å“å¸‚åœºè´­ä¹°ã€‚
```

```
User: æ¥å‡ å¥æˆè¯­
Assistant: â€œä¸€æ—¥ä¸‰å°ºâ€æ˜¯æŒ‡è¿‡æ—¶é—´ä¸€åˆ‡éƒ½ä¼šæ”¹å˜ã€‚
```

```
User: åˆ—å‡º5ç§å¯ä»¥æ”¹å–„ç¡çœ è´¨é‡çš„æ–¹æ³•
Assistant:
1. ä¿æŒè§„å¾‹çš„ç¡çœ æ—¶é—´è¡¨
2. é¿å…ä¸‹åˆæ‘„å…¥å’–å•¡å› 
3. é¿å…ç¡å‰æ‘„å…¥é…’ç²¾
4. é¿å…ç¡å‰æ‘„å…¥é«˜ç³–é¥®é£Ÿ
5. æ¯å¤©è‡³å°‘è¿›è¡Œ30åˆ†é’Ÿçš„è¿åŠ¨
```

```
User: å›ç­”ä¸€ä¸ªå¼€æ”¾æ€§çš„é—®é¢˜ï¼šäººç±»å­˜åœ¨çš„æ„ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ
Assistant: äººç±»å­˜åœ¨çš„æ„ä¹‰æ˜¯ä¸€ä¸ªå¤æ‚çš„å“²å­¦å’Œå“²å­¦é—®é¢˜ï¼Œå› ä¸ºäººç±»å­˜åœ¨äºè‡ªç„¶ç•Œä¸­ï¼Œæˆ‘ä»¬ä½œä¸ºäººç±»æœ‰è´£ä»»å’Œä¹‰åŠ¡æ¥ä¿æŠ¤åœ°çƒå’Œç”Ÿç‰©ï¼Œè€Œæˆ‘ä»¬ä½œä¸ºäººç±»ä¹Ÿæœ‰è´£ä»»æ¥å‘å±•è‡ªå·±çš„ç²¾ç¥å’Œæ€æƒ³ï¼Œæ¥åˆ›é€ æ›´å¥½çš„æœªæ¥ã€‚
æˆ‘ä»å“²å­¦è§’åº¦æ¥çœ‹ï¼Œäººç±»å­˜åœ¨çš„æ„ä¹‰æ˜¯é€šè¿‡æˆ‘ä»¬è‡ªå·±çš„æ€æƒ³å’Œè¡ŒåŠ¨ï¼Œæ¥å®ç°æˆ‘ä»¬å¯¹ç²¾ç¥å’Œç²¾ç¥çš„è¿½æ±‚ã€‚é€šè¿‡æˆ‘ä»¬è‡ªå·±çš„åŠªåŠ›å’Œåˆ›é€ åŠ›ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›é€ å‡ºæ›´å¤šçš„è‰ºæœ¯ã€æ–‡åŒ–å’Œæ€æƒ³ï¼Œæ¥ä¿ƒè¿›äººç±»çš„è¿›æ­¥å’Œå‘å±•ã€‚
```

```
User: è§£é‡Šä»¥ä¸‹ç¼©å†™ï¼š JSONã€CEOã€PDFã€HTMLã€NASA
Assistant:
JSONï¼ˆJavaScript Object Notationï¼‰ï¼šJavaScriptå¯¹è±¡çš„è¡¨ç¤ºæ³•ï¼Œç”¨æ¥åœ¨JavaScriptå’Œè®¸å¤šå…¶ä»–ç¼–ç¨‹è¯­è¨€ä¹‹é—´ä¼ é€’æ•°æ®ã€‚
CEOï¼ˆChief Executive Officerï¼‰ï¼šé¦–å¸­æ‰§è¡Œå®˜ï¼ŒæŒ‡ä¼ä¸šçš„é«˜çº§ç®¡ç†äººå‘˜ï¼Œé€šå¸¸æ˜¯æŒ‡ä¼ä¸šçš„ä¸»å¸­æˆ–æ€»è£ã€‚
PDFï¼ˆPortable Document Formatï¼‰ï¼šå¯ç§»æ¤æ–‡æ¡£æ ¼å¼ï¼Œæ˜¯ä¸€ç§ç”µå­æ–‡æ¡£æ ¼å¼ï¼Œå¯ä»¥ç¡®ä¿æ–‡æ¡£åœ¨ä¸åŒçš„æ“ä½œç³»ç»Ÿå’Œè®¾å¤‡ä¸Šéƒ½èƒ½ä»¥ä¸€è‡´çš„æ–¹å¼å‘ˆç°ã€‚
HTMLï¼ˆHyperText Markup Languageï¼‰ï¼šè¶…æ–‡æœ¬æ ‡è®°è¯­è¨€ï¼Œæ˜¯ç½‘é¡µå†…å®¹çš„æ ‡è®°è¯­è¨€ï¼Œç”¨æ¥å®šä¹‰ç½‘é¡µçš„ç»“æ„å’Œå†…å®¹ã€‚
NASAï¼ˆNational Aeronautics and Space Administrationï¼‰ï¼šç¾å›½å›½å®¶èˆªç©ºèˆªå¤©å±€ï¼Œæ˜¯ç¾å›½æ”¿åºœçš„å®‡èˆªæœºæ„ï¼Œè´Ÿè´£ç¾å›½åœ¨å¤ªç©ºå’Œèˆªç©ºæ–¹é¢çš„ç ”ç©¶å’Œå‘å±•ã€‚
```

## Function call

**User**:
````
ä»¥ä¸‹æ˜¯æ‚¨å¯ä»¥ä½¿ç”¨çš„å·¥å…·åˆ—è¡¨:

```python
def internet_search(query: str):
    \"\"\"
    Returns a list of relevant document snippets for a textual query retrieved from the internet

    Args:
        query (str): Query to search the internet with
    \"\"\"
    pass
```

```python
def directly_answer():
    \"\"\"
    Calls a standard (un-augmented) AI chatbot to generate a response given the conversation history
    \"\"\"
    pass
```

å†™ 'Action:'ï¼Œåè·Ÿè¦è°ƒç”¨çš„ JSON ä¸­çš„æ“ä½œåˆ—è¡¨ï¼Œä¾‹å¦‚.
Action:
```json
[
    {
        "tool_name": "tool name (one of [internet_search, directly_answer])",
        "parameters": "the input to the tool"
    }
]
```

å¸®æˆ‘æ‰¾åˆ°ä»Šå¤©çš„æ–°é—»æœ‰å“ªäº›:
````

**Response**:
```
Action:
[
  {
    "tool_name": "internet_search", 
    "parameters": "ä»Šå¤©æœ‰å“ªäº›æ–°é—»"
  }
]
```
